# AI Network Recommendation Engine

This project is a lightweight AI system that recommends professional connections based on shared history, work, and network proximity. It features a **LangChain agent** powered by Google's Gemini model, which intelligently routes user queries to the best retrieval tool—SQL, vector, or graph—based on a natural language prompt. The entire experience is delivered through an intuitive Streamlit web interface.

## Core Technologies & Tools

-   **AI & Machine Learning**:
    -   **LangChain**: The core framework for building the LLM-powered agent.
    -   **Google Gemini**: The language model used for natural language understanding and query routing.
    -   `sentence-transformers`: For generating high-quality vector embeddings of user bios.
    -   **spaCy**: For named entity recognition (NER) to identify user names in prompts.
-   **Databases**:
    -   **DuckDB**: An in-process SQL OLAP database for storing and querying structured user data.
    -   **Neo4j**: A graph database for modeling and querying network connections.
    -   **Qdrant**: A vector database for efficient semantic similarity search.
-   **UI**:
    -   **Streamlit**: For building the interactive web application.

## About the Data

The dataset used in this project is **entirely synthetic and was generated by ChatGPT** for demonstration purposes. It includes:

1.  **Structured Data (`data/structured/users.csv`):** A CSV file containing 100 fictional user profiles with fields like `name`, `email`, `company`, `school`, `title`, and `location`.
2.  **Unstructured Data (`data/unstructured/`):** A collection of sample resumes and bios in text format, used to simulate real-world, unstructured data sources.

## Setup & Installation

1.  **Clone the repository:**
    ```bash
    git clone <repository-url>
    cd network-recommendation-engine
    ```

2.  **Create and activate a Python virtual environment:**
    *This project requires Python 3.12.*
    ```bash
    python3.12 -m venv venv
    source venv/bin/activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Set up environment variables:**
    Create a `.env` file in the project root and add your credentials. You will need a free [Neo4j Aura](https://neo4j.com/cloud/aura/) instance and a [Google AI Studio API Key](https://aistudio.google.com/app/apikey).
    ```
    # .env
    NEO4J_URI="neo4j+s://<your-neo4j-uri>"
    NEO4J_USER="neo4j"
    NEO4J_PASSWORD="<your-neo4j-password>"
    GOOGLE_API_KEY="<your-google-api-key>"
    ```

## How to Run

1.  **Run the data preprocessing pipeline:**
    This script will parse all raw data, load it into the databases (DuckDB, Neo4j), and create the vector search index (Qdrant). Make it executable first.
    ```bash
    chmod +x preprocess_data.sh
    ./preprocess_data.sh
    ```

2.  **Launch the Streamlit application:**
    ```bash
    streamlit run ui/app.py
    ```

Now, open your web browser to the local Streamlit URL (usually `http://localhost:8501`) to start getting recommendations!

**Example Prompts:**
- `Find users who work at Google`
- `Find connections for Alice Heart`
- `Show me profiles similar to Bob Johnson`

-   **Databases**:
    -   **Neo4j**: Graph database for storing and querying network relationships.
    -   **Qdrant**: Vector database for efficient semantic similarity search.
    -   **DuckDB**: Embedded SQL database for handling structured data.
-   **Python Libraries**:
    -   `langchain-google-genai`: Google Generative AI integration for LangChain.
    -   `neo4j`: Official Python driver for Neo4j.
    -   `qdrant-client`: Client for interacting with Qdrant.
    -   `duckdb`: Python API for DuckDB.
    -   `pandas`: For data manipulation and analysis.
    -   `PyMuPDF`: For extracting text from PDF files.
    -   `python-dotenv`: For managing environment variables.
